{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szpBAhPxPdrB"
      },
      "outputs": [],
      "source": [
        "# Standard Import Stuff\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score, silhouette_score)\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive/Data Files/cleaned_rq1.csv\"\n",
        "\n",
        "rq1_df = pd.read_csv(path)\n",
        "print(rq1_df.shape)\n",
        "print(rq1_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# There are negative values for resolution time. Not possible, potential datetime errors from pulled dataset.\n",
        "# Will set all values < 0 to = 0, assuming same day resolution.\n",
        "# Additionally, with a mean of 11.97 for resolution_days and a stdev of 61.08, that is heavily skewed by high outliers\n",
        "# Due to the heavily-right skewing, will perform log transform to lessen the influence of outliers.\n",
        "\n",
        "rq1_df.loc[rq1_df['resolution_days'] < 0, 'resolution_days'] = 0\n",
        "\n",
        "rq1_df.describe(include='all')"
      ],
      "metadata": {
        "id": "JrJcG2NuPo7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assumption is that the post-pandemic data will differ greatly and thus skew the train/test split.\n",
        "# To remedy this we will split the dataframe into our two timelines suggested from EDA: 2016-2019 and 2020-2023.\n",
        "\n",
        "rq1_df['occurred_datetime'] = pd.to_datetime(rq1_df['occurred_datetime'])\n",
        "rq1_df['report_datetime'] = pd.to_datetime(rq1_df['report_datetime'])\n",
        "\n",
        "rq1_time1_df = rq1_df[(rq1_df['occurred_datetime'].dt.year >= 2016) & (rq1_df['occurred_datetime'].dt.year <= 2019)]\n",
        "rq1_time2_df = rq1_df[(rq1_df['occurred_datetime'].dt.year >= 2020) & (rq1_df['occurred_datetime'].dt.year <= 2023)]\n",
        "\n",
        "print(\"2016-2019 Shape: \", rq1_time1_df.shape)\n",
        "print(\"2020-2023 Shape: \", rq1_time2_df.shape)"
      ],
      "metadata": {
        "id": "NtFY28oAPsyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now to prepare for modeling we have to split the data for training and testing\n",
        "# Because we are planning to compare the performance of multiple models, we will use a 70/30 split\n",
        "# We are operating off of 2 different dataframes now, 3 if we include the original which should remain unchanged.\n",
        "# Features to choose from are 'offense', 'family_violence', 'location_type', 'tract_geoid'\n",
        "# Target variable is 'resolution_days'\n",
        "# Random state is Pi, everyone loves Pi.\n",
        "\n",
        "X1 = pd.get_dummies(rq1_time1_df[['offense', 'location_type', 'tract_geoid']], drop_first=True)\n",
        "y1 = np.log1p(rq1_time1_df['resolution_days'])\n",
        "\n",
        "\n",
        "X2 = rq1_time2_df[['offense']]\n",
        "y2 = np.log1p(rq1_time2_df['resolution_days'])\n",
        "\n",
        "X2_encoded = pd.get_dummies(X2, columns=['offense'] )\n",
        "\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3, random_state=314)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_encoded, y2, test_size=0.3, random_state=314)"
      ],
      "metadata": {
        "id": "SA1io8FbPvZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}